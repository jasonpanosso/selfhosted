---
monitoring:
  enabled: true
  createPrometheusRules: true

toolbox:
  enabled: true

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.2
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10
  upgradeOSDRequiresHealthyPGs: false
  logCollector:
    enabled: false
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: rook
        enabled: true
      - name: pg_autoscaler
        enabled: true
      - name: insights
        enabled: true
  resources:
    api:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        memory: 512Mi
    mgr:
      limits:
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 1Gi
    mon:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        memory: 2Gi
    osd:
      requests:
        cpu: 500m
        memory: 3Gi
      limits:
        cpu: 500m
        memory: 3Gi
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
    api: system-cluster-critical
    exporter: system-node-critical
    crashcollector: system-node-critical
  dashboard:
    enabled: true
    urlPrefix: /
    port: 7000
    ssl: false
    prometheusEndpoint: http://prometheus-operated.observability.svc.cluster.local:9090
  network:
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false
      requireMsgr2: false
  removeOSDsIfOutAndSafeToRemove: true

  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    allowDeviceClassUpdate: true # whether to allow changing the device class of an OSD after it is created
    allowOsdCrushWeightUpdate: true # whether to allow resizing the OSD crush weight after osd pvc is increased
    scheduleAlways: false
    onlyApplyOSDPlacement: false
    nodes:
      - name: worker-1
        deviceFilter: ^nvme
      - name: worker-2
        deviceFilter: ^nvme
      - name: worker-3
        deviceFilter: ^nvme
      - name: worker-4
        deviceFilter: ^nvme
      - name: worker-5
        devices:
          - name: /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEJSXBUS
          - name: /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEJPGEBS
          - name: /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK2338P4H4X3MC

cephBlockPools:
  - name: builtin-mgr
    spec:
      failureDomain: host
      deviceClass: nvme
      enableCrushUpdates: true
      replicated:
        name: ".mgr"
        size: 3
        requireSafeReplicaSize: true
      enableRBDStats: true
      parameters:
        compression_mode: none
      mirroring:
        enabled: false
    storageClass:
      enabled: false
  - name: replica-pool
    spec:
      failureDomain: host
      deviceClass: nvme
      enableCrushUpdates: true
      replicated:
        size: 3
        requireSafeReplicaSize: true
      enableRBDStats: true
      parameters:
        compression_mode: none
      mirroring:
        enabled: false
    storageClass:
      enabled: true
      isDefault: false
      name: rook-ceph-block
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      mountOptions: ["discard"]
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4
  - name: hdd-replica-pool
    spec:
      failureDomain: osd
      deviceClass: hdd
      enableCrushUpdates: true
      replicated:
        size: 3
        requireSafeReplicaSize: true
      parameters:
        compression_mode: none
      mirroring:
        enabled: false
    storageClass:
      enabled: true
      isDefault: false
      name: rook-ceph-block-hdd
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      mountOptions: ["discard"]
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4
cephBlockPoolsVolumeSnapshotClass:
  enabled: true
  isDefault: false
  name: csi-ceph-blockpool
  deletionPolicy: Delete

cephFileSystems:
  - name: hddfs
    spec:
      metadataPool:
        failureDomain: host
        enableCrushUpdates: true
        deviceClass: nvme
        replicated:
          size: 3
          requireSafeReplicaSize: true
      dataPools:
        - name: replicated
          failureDomain: osd
          enableCrushUpdates: true
          deviceClass: hdd
          replicated:
            size: 3
            requireSafeReplicaSize: true
      preserveFilesystemOnDelete: true
      metadataServer:
        activeCount: 1
        activeStandby: true
        priorityClassName: system-cluster-critical
        resources:
          limits:
            cpu: 500m
            memory: 1024Mi
          requests:
            cpu: 500m
            memory: 1024Mi
    storageClass:
      enabled: true
      isDefault: false
      name: rook-cephfs-hdd
      pool: hddfs-replicated
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
cephFileSystemVolumeSnapshotClass:
  enabled: false

cephObjectStores:
  - name: my-store
    spec:
      metadataPool:
        failureDomain: host
        enableCrushUpdates: true
        deviceClass: nvme
        replicated:
          size: 3
      dataPool:
        failureDomain: osd
        enableCrushUpdates: true
        deviceClass: hdd
        # For production it is recommended to use more chunks, such as 4+2 or 8+4
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
      preservePoolsOnDelete: true
      gateway:
        hostNetwork: false
        port: 8080
        instances: 2
        priorityClassName: system-cluster-critical
        resources:
          requests:
            cpu: 100m
            memory: 1Gi
          limits:
            memory: 2Gi
      healthCheck:
        bucket:
          interval: 60s
      hosting:
        advertiseEndpoint:
          dnsName: rook-ceph-rgw-my-store.rook-ceph.svc.cluster.local
          port: 8080
          useTls: false
        dnsNames:
          - s3.${domain}.${tld}
          - rook-ceph-rgw-my-store.rook-ceph.svc.cluster.local
    storageClass:
      enabled: true
      isDefault: false
      name: rook-ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: Immediate
