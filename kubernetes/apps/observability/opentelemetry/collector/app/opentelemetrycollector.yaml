---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: node
spec:
  mode: daemonset
  serviceAccount: otel-collector-opentelemetry-collector
  priorityClassName: system-node-critical
  targetAllocator:
    enabled: true
    allocationStrategy: per-node
    serviceAccount: opentelemetry-targetallocator
    prometheusCR:
      enabled: true
      scrapeInterval: 15s
      podMonitorSelector: {}
      serviceMonitorSelector: {}
      probeSelector: {}
      scrapeConfigSelector: {}
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 512Mi
  env:
    - name: GOMEMLIMIT
      value: 500MiB
    - name: OTEL_RESOURCE_ATTRIBUTES
      value: "k8s.cluster.name=prod"
    - name: OTEL_K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
  tolerations:
    - operator: Exists
      effect: NoSchedule
  nodeSelector:
    kubernetes.io/os: linux
  config:
    exporters:
      debug/metrics: {}
      debug/logs: {}
      debug/traces: {}
      otlp:
        endpoint: gateway-collector.observability.svc.cluster.local:4317
        tls:
          insecure: true

    processors:
      batch:
        send_batch_max_size: 1500
        send_batch_size: 1000
        timeout: 1s
      memory_limiter:
        check_interval: 1s
        limit_percentage: 80
        spike_limit_percentage: 15
      resourcedetection/env:
        detectors: [env]
        timeout: 2s
        override: false
      # have to set cluster as an attribute, not resource for kube-prometheus-stack
      # rules/alerts/dashboards due to resource flattening to attributes upon
      # export to prometheus, overwriting the cnpg cluster label
      attributes/cluster_default:
        actions:
          - key: cluster
            value: prod
            action: insert
      k8sattributes:
        filter:
          node_from_env_var: OTEL_K8S_NODE_NAME
        passthrough: false
        extract:
          otel_annotations: true
          labels:
            - from: pod
              key: app.kubernetes.io/name
              tag_name: service.name
            - from: pod
              key: k8s-app
              tag_name: service.name
            - from: pod
              key: app.kubernetes.io/instance
              tag_name: k8s.app.instance
            - from: pod
              key: app.kubernetes.io/version
              tag_name: service.version
            - from: pod
              key: app.kubernetes.io/component
              tag_name: k8s.app.component
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.container.name
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
            - k8s.cluster.uid
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
              - from: resource_attribute
                name: k8s.node.name
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
          - sources:
              - from: connection

    receivers:
      prometheus:
        config:
          # target allocator
          scrape_configs: []

      filelog:
        exclude: []
        include:
          - /var/log/pods/*/*/*.log
        include_file_name: false
        include_file_path: true
        operators:
          - id: container-parser
            max_log_size: 102400
            type: container
        retry_on_failure:
          enabled: true
        start_at: end

    service:
      pipelines:
        logs:
          exporters:
            # - debug/logs
            - otlp
          processors:
            - memory_limiter
            - k8sattributes
            - resourcedetection/env
            - batch
          receivers:
            - filelog
        metrics:
          exporters:
            # - debug/metrics
            - otlp
          processors:
            - memory_limiter
            - k8sattributes
            - resourcedetection/env
            - attributes/cluster_default
            - batch
          receivers:
            - prometheus
  volumeMounts:
    - mountPath: /var/log/pods
      name: varlogpods
      readOnly: true

  volumes:
    - hostPath:
        path: /var/log/pods
      name: varlogpods
---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: cluster
spec:
  mode: deployment
  priorityClassName: system-cluster-critical
  replicas: 2
  serviceAccount: otel-collector-opentelemetry-collector
  resources:
    limits:
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 256Mi
  env:
    - name: GOMEMLIMIT
      value: 250MiB
    - name: OTEL_RESOURCE_ATTRIBUTES
      value: "k8s.cluster.name=prod"
  config:
    extensions:
      k8s_leader_elector:
        auth_type: serviceAccount
        lease_name: otel-collector-daemonset-leader
        lease_namespace: observability

    exporters:
      debug: {}
      otlp:
        endpoint: gateway-collector.observability.svc.cluster.local:4317
        tls:
          insecure: true

    processors:
      batch:
        send_batch_max_size: 1500
        send_batch_size: 1000
        timeout: 1s
      memory_limiter:
        check_interval: 1s
        limit_percentage: 80
        spike_limit_percentage: 15
      resourcedetection/env:
        detectors: [env]
        timeout: 2s
        override: false
      k8sattributes:
        passthrough: false
        extract:
          otel_annotations: true
          labels:
            - from: pod
              key: app.kubernetes.io/name
              tag_name: service.name
            - from: pod
              key: k8s-app
              tag_name: service.name
            - from: pod
              key: app.kubernetes.io/instance
              tag_name: k8s.app.instance
            - from: pod
              key: app.kubernetes.io/version
              tag_name: service.version
            - from: pod
              key: app.kubernetes.io/component
              tag_name: k8s.app.component
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.container.name
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
            - k8s.cluster.uid
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
              - from: resource_attribute
                name: k8s.node.name
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
          - sources:
              - from: connection

    receivers:
      k8s_cluster:
        auth_type: serviceAccount
        collection_interval: 10s
        k8s_leader_elector: k8s_leader_elector
        allocatable_types_to_report:
          - cpu
          - memory
          - storage
        node_conditions_to_report:
          - Ready
          - MemoryPressure
          - DiskPressure
          - NetworkUnavailable
      k8sobjects:
        k8s_leader_elector: k8s_leader_elector
        auth_type: serviceAccount
        objects:
          - name: events
            group: events.k8s.io
            mode: watch
            exclude_watch_type:
              - DELETED

    service:
      extensions:
        - k8s_leader_elector
      pipelines:
        logs:
          exporters:
            # - debug
            - otlp
          processors:
            - memory_limiter
            - k8sattributes
            - resourcedetection/env
            - batch
          receivers:
            - k8sobjects
        metrics:
          exporters:
            # - debug
            - otlp
          processors:
            - memory_limiter
            - k8sattributes
            - resourcedetection/env
            - batch
          receivers:
            - k8s_cluster
---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: gateway
spec:
  mode: deployment
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.127.0
  priorityClassName: system-cluster-critical
  replicas: 2
  serviceAccount: otel-collector-opentelemetry-collector
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 500m
      memory: 512Mi
  env:
    - name: GOMEMLIMIT
      value: 500MiB
  config:
    exporters:
      debug: {}
      prometheusremotewrite:
        endpoint: http://prometheus-operated.observability.svc.cluster.local:9090/api/v1/write
        add_metric_suffixes: false
        resource_to_telemetry_conversion:
          enabled: true
        tls:
          insecure: true
      # otlphttp/prometheus:
      #   endpoint: http://prometheus-operated.observability.svc.cluster.local:9090/api/v1/otlp
      #   tls:
      #     insecure: true
      otlphttp/loki:
        endpoint: http://loki-gateway.observability.svc.cluster.local/otlp
        tls:
          insecure: true

    processors:
      batch:
        send_batch_max_size: 1500
        send_batch_size: 1000
        timeout: 1s
      memory_limiter:
        check_interval: 1s
        limit_percentage: 80
        spike_limit_percentage: 15

    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317

    service:
      pipelines:
        logs:
          exporters:
            # - debug
            - otlphttp/loki
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
        metrics:
          exporters:
            # - debug
            - prometheusremotewrite
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
